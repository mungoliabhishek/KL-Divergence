# KL-Divergence

KL Divergence is just the difference between a fitted distribution and actual distribution, i.e. the difference between cross-entropy and entropy. It can also be looked as to how much the two distributions differ.

Find more about it
Part 1 - https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e
Part 2 - https://towardsdatascience.com/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d
